{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89cfe80-6a86-4c0d-946d-fe6da9157d37",
   "metadata": {},
   "source": [
    "# Data processcing step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61969826",
   "metadata": {},
   "source": [
    "### 1. Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb231d62-d6a0-4f3b-b00d-1a7abc58adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data processcing DE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6c60d",
   "metadata": {},
   "source": [
    "### 2. Read the student list file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65db302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "+---+--------------------+\n",
      "|_c0|                 _c1|\n",
      "+---+--------------------+\n",
      "|  1|          Mai Đức An|\n",
      "|  2|      Nguyễn Mai Anh|\n",
      "|  3|   Ngô Ngọc Tuấn Anh|\n",
      "|  4|      Trần Trung Anh|\n",
      "|  5|       Trần Ngọc Bảo|\n",
      "|  6|  Nguyễn Vũ Hòa Bình|\n",
      "|  7|    Nguyễn Thành Đạt|\n",
      "|  8|        Đỗ Thành Đạt|\n",
      "|  9|    Nguyễn Khoa Đoàn|\n",
      "| 10|    Nguyễn Quốc Dũng|\n",
      "| 11|     Đường Minh Quân|\n",
      "| 12|   Dương Quang Giang|\n",
      "| 13|    Nguyễn Minh Hiếu|\n",
      "| 14|        Ngô Phi Hùng|\n",
      "| 15|Nguyễn Đình Thiên...|\n",
      "| 16|        Đỗ Doãn Khắc|\n",
      "| 17|      Châu Minh Khải|\n",
      "| 18|      Phạm Đình Khôi|\n",
      "| 19|        Lê Bảo Khánh|\n",
      "| 20|        Lê Minh Phúc|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs_path_student_list = \"hdfs://namenode:9000/raw_zone/fact/student_list\"\n",
    "\n",
    "# Read Parquet files from HDFS path into a DataFrame\n",
    "student_df = spark.read.csv(hdfs_path_student_list)\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "student_df.printSchema()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "student_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd36ba",
   "metadata": {},
   "source": [
    "### 3. Read the student activity file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738aeff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studentCode: integer (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- numberOfFiles: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+-----------+--------+-------------+---------+\n",
      "|studentCode|activity|numberOfFiles|timestamp|\n",
      "+-----------+--------+-------------+---------+\n",
      "|          4|   write|            7|6/10/2024|\n",
      "|         33|    read|            5|6/12/2024|\n",
      "|         33| execute|            1|6/13/2024|\n",
      "|          6|   write|            6|6/15/2024|\n",
      "|         24| execute|            8|6/12/2024|\n",
      "|         22|   write|            2|6/12/2024|\n",
      "|         31|   write|            9|6/13/2024|\n",
      "|          8|   write|            4|6/13/2024|\n",
      "|         21|    read|            5|6/12/2024|\n",
      "|         26| execute|            2|6/10/2024|\n",
      "|         24|    read|           10|6/12/2024|\n",
      "|         10|    read|            6|6/15/2024|\n",
      "|         20|   write|            7|6/14/2024|\n",
      "|         14| execute|            7|6/11/2024|\n",
      "|          5| execute|            1|6/10/2024|\n",
      "|          3|    read|            9|6/13/2024|\n",
      "|         11|   write|            7|6/11/2024|\n",
      "|          7|    read|            1|6/15/2024|\n",
      "|         22|   write|            9|6/14/2024|\n",
      "|          4| execute|            8|6/13/2024|\n",
      "+-----------+--------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs_path_activity = \"hdfs://namenode:9000/raw_zone/fact/activity\"\n",
    "\n",
    "# Read Parquet files from HDFS path into a DataFrame\n",
    "activity_df = spark.read.parquet(hdfs_path_activity)\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "activity_df.printSchema()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7c1fc-7cde-4df7-9cda-480f15ce8e88",
   "metadata": {},
   "source": [
    "### Convert date datatype from string to date, sort by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b314c179-66b7-46f9-b6c3-a475da8f1ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------+----------+\n",
      "|studentCode|activity|numberOfFiles| timestamp|\n",
      "+-----------+--------+-------------+----------+\n",
      "|          5| execute|            1|2024-06-10|\n",
      "|         35| execute|            4|2024-06-10|\n",
      "|         38| execute|            6|2024-06-10|\n",
      "|         22| execute|            6|2024-06-10|\n",
      "|          8|   write|            7|2024-06-10|\n",
      "|         33|   write|            3|2024-06-10|\n",
      "|          4|    read|            4|2024-06-10|\n",
      "|         24|   write|           10|2024-06-10|\n",
      "|          1|    read|            9|2024-06-10|\n",
      "|         31|    read|            1|2024-06-10|\n",
      "|          3| execute|            4|2024-06-10|\n",
      "|         26| execute|            2|2024-06-10|\n",
      "|         10|   write|            5|2024-06-10|\n",
      "|         36|    read|            6|2024-06-10|\n",
      "|         17| execute|            5|2024-06-10|\n",
      "|          5| execute|            1|2024-06-10|\n",
      "|         16|    read|            6|2024-06-10|\n",
      "|         36|    read|            5|2024-06-10|\n",
      "|         25|    read|            7|2024-06-10|\n",
      "|         39|   write|            1|2024-06-10|\n",
      "+-----------+--------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "activity_df = activity_df.withColumn(\"timestamp\", to_date(activity_df.timestamp, \"M/d/yyyy\"))\n",
    "\n",
    "activity_df = activity_df.sort(\"timestamp\")\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0a3cebc-a1a8-4eda-977d-1fd9bdae2273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------+----------+---+----------+\n",
      "|studentCode|activity|numberOfFiles| timestamp|_c0|       _c1|\n",
      "+-----------+--------+-------------+----------+---+----------+\n",
      "|         37|    read|            8|2024-06-10| 37|Đào Anh Vũ|\n",
      "|         37| execute|            6|2024-06-15| 37|Đào Anh Vũ|\n",
      "|         37|    read|            5|2024-06-15| 37|Đào Anh Vũ|\n",
      "|         37| execute|            9|2024-06-12| 37|Đào Anh Vũ|\n",
      "|         37|    read|           10|2024-06-11| 37|Đào Anh Vũ|\n",
      "|         37| execute|            9|2024-06-10| 37|Đào Anh Vũ|\n",
      "|         37|   write|            3|2024-06-11| 37|Đào Anh Vũ|\n",
      "|         37|   write|            4|2024-06-11| 37|Đào Anh Vũ|\n",
      "|         37|   write|            5|2024-06-10| 37|Đào Anh Vũ|\n",
      "|         37|    read|            5|2024-06-15| 37|Đào Anh Vũ|\n",
      "|         37| execute|            1|2024-06-13| 37|Đào Anh Vũ|\n",
      "+-----------+--------+-------------+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = activity_df.join(student_df, activity_df.studentCode == student_df._c0).filter(activity_df.studentCode == 37)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23f2c1f2-2dae-4f1a-be3f-d7b5a0127855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+-----------+----------+\n",
      "|activity| timestamp|student_name|studentCode|totalFiles|\n",
      "+--------+----------+------------+-----------+----------+\n",
      "| execute|2024-06-10|  Đào Anh Vũ|         37|         9|\n",
      "| execute|2024-06-12|  Đào Anh Vũ|         37|         9|\n",
      "| execute|2024-06-13|  Đào Anh Vũ|         37|         1|\n",
      "| execute|2024-06-15|  Đào Anh Vũ|         37|         6|\n",
      "|    read|2024-06-10|  Đào Anh Vũ|         37|         8|\n",
      "|    read|2024-06-11|  Đào Anh Vũ|         37|        10|\n",
      "|    read|2024-06-15|  Đào Anh Vũ|         37|        10|\n",
      "|   write|2024-06-10|  Đào Anh Vũ|         37|         5|\n",
      "|   write|2024-06-11|  Đào Anh Vũ|         37|         7|\n",
      "+--------+----------+------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, sum\n",
    "\n",
    "grouped_df = joined_df.groupBy(\"activity\", \"timestamp\").agg(first(\"_c1\").alias(\"student_name\"), \n",
    "    first(\"studentCode\").alias(\"studentCode\"), sum(\"numberOfFiles\").alias(\"totalFiles\"))\n",
    "\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "687a1a4b-614e-4994-a4ce-402757cc312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+--------+----------+\n",
      "|timestamp|studentCode|student_name|activity|totalFiles|\n",
      "+---------+-----------+------------+--------+----------+\n",
      "| 20240610|         37|  Đào Anh Vũ| execute|         9|\n",
      "| 20240610|         37|  Đào Anh Vũ|    read|         8|\n",
      "| 20240610|         37|  Đào Anh Vũ|   write|         5|\n",
      "| 20240611|         37|  Đào Anh Vũ|    read|        10|\n",
      "| 20240611|         37|  Đào Anh Vũ|   write|         7|\n",
      "| 20240612|         37|  Đào Anh Vũ| execute|         9|\n",
      "| 20240613|         37|  Đào Anh Vũ| execute|         1|\n",
      "| 20240615|         37|  Đào Anh Vũ| execute|         6|\n",
      "| 20240615|         37|  Đào Anh Vũ|    read|        10|\n",
      "+---------+-----------+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "grouped_df = grouped_df.select(\"timestamp\", \"studentCode\", \"student_name\", \"activity\", \"totalFiles\").sort(\"timestamp\")\n",
    "grouped_df = grouped_df.withColumn(\"timestamp\", date_format(\"timestamp\", \"yyyyMMdd\"))\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e17a978-5261-45b8-982e-56231a229e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully save data to HDFS: hdfs://namenode:9000/raw_zone/fact/output\n"
     ]
    }
   ],
   "source": [
    "hdfs_save_path = \"hdfs://namenode:9000/raw_zone/fact/output\"\n",
    "\n",
    "grouped_df.write.csv(hdfs_save_path, header=True)\n",
    "\n",
    "print(\"Successfully save data to HDFS:\", hdfs_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f57d4a3c-f1d6-4c66-9b50-dc0857c03726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data to Docker volume\n",
      "./output/part-00000-ef176a2e-2f7e-4de0-ab0d-ea36af3dbfd7-c000.csv\n",
      "./output/output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "grouped_df.write.csv(\"./output\", mode=\"overwrite\", header=True)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Successfully saved data to Docker volume\")\n",
    "\n",
    "for filename in os.listdir(\"./output\"):\n",
    "    file_path = os.path.join(\"./output\", filename)\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        print(file_path)\n",
    "        split_array = file_path.split(\"/\")\n",
    "        split_array[-1] = \"output.csv\"\n",
    "        new_file_path = \"/\".join(split_array)\n",
    "        print(new_file_path)\n",
    "        os.rename(file_path, new_file_path)\n",
    "    else:\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a5e9b-b5ee-4525-a227-1b33549856b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
